{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 581888\n"
     ]
    }
   ],
   "source": [
    "path = '1661-0.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing - This will divide the dataset into  tokens.without special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'w+')\n",
    "words = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = np.unique(words)\n",
    "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'w', 'w', 'w', 'w']\n",
      "www\n"
     ]
    }
   ],
   "source": [
    "WORD_LENGTH = 5\n",
    "prev_words = []\n",
    "next_words = []\n",
    "for i in range(len(words) - WORD_LENGTH):\n",
    "    prev_words.append(words[i:i + WORD_LENGTH])\n",
    "    next_words.append(words[i + WORD_LENGTH])\n",
    "print(prev_words[0])\n",
    "print(next_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
    "labels = np.zeros((len(next_words), len(unique_words)), dtype=bool)\n",
    "for i, each_words in enumerate(prev_words):\n",
    "    for j, each_word in enumerate(each_words):\n",
    "        features[i, j, unique_word_index[each_word]] = 1\n",
    "    labels[i, unique_word_index[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))\n",
    "model.add(Dense(len(unique_words)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "86/86 [==============================] - 3s 16ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1460 - val_accuracy: 0.9844\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 9.6417e-04 - accuracy: 0.9999 - val_loss: 0.1598 - val_accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1527 - val_accuracy: 0.9844\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1433 - val_accuracy: 0.9844\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 9.9255e-04 - accuracy: 0.9999 - val_loss: 0.1542 - val_accuracy: 0.9844\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1565 - val_accuracy: 0.9844\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1500 - val_accuracy: 0.9844\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 9.8709e-04 - accuracy: 0.9999 - val_loss: 0.1572 - val_accuracy: 0.9844\n",
      "Epoch 9/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1585 - val_accuracy: 0.9844\n",
      "Epoch 10/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1598 - val_accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(features, labels, validation_split=0.05, batch_size=128, epochs=10, shuffle=True)\n",
    "hist = model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saurav\\.conda\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save('nwp.h5')\n",
    "pickle.dump(hist, open(\"history.p\", \"wb\"))\n",
    "model = load_model('nwp.h5')\n",
    "history = pickle.load(open(\"history.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Starting with: Once upon a time\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "---\n",
      "Completed sentence: \"Once upon a time w w w w w w w w w w w w w w w w w w w w w w\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def prepare_input(text):\n",
    "    x = np.zeros((1, WORD_LENGTH, len(unique_words)))\n",
    "    words = text.split()\n",
    "    for t in range(min(len(words), WORD_LENGTH)):\n",
    "        word = words[t]\n",
    "        index = unique_word_index.get(word, None)\n",
    "        if index is not None:\n",
    "            x[0, t, index] = 1\n",
    "    return x\n",
    "\n",
    "def predict_next_word(model, text): \n",
    "    input_vec = prepare_input(text)\n",
    "    prediction = model.predict(input_vec)[0]\n",
    "    next_word_probabilities = prediction / np.sum(prediction)\n",
    "    return random.choices(list(unique_words), weights=next_word_probabilities)[0]\n",
    "\n",
    "# Generate a sentence using the trained model\n",
    "sentence = \"Once upon a time\"\n",
    "print(\"---\")\n",
    "print(f\"Starting with: {sentence}\")\n",
    "for _ in range(len(sentence.split()), len(sentence) + 10):  # Iterate for a few more words than the initial sentence length\n",
    "    word = predict_next_word(model, sentence)\n",
    "    sentence += f\" {word}\"\n",
    "    if word == \".\":\n",
    "        break\n",
    "print(\"---\")\n",
    "print(f\"Completed sentence: \\\"{sentence}\\\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
